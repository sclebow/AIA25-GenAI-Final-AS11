# -*- coding: utf-8 -*-
"""AIA25_GenAI_Group11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LDkyZCWRleEj1TEk3qQbHF_JPXKuQRwG

# Gradio Demo: live_dashboard
### This demo shows how you can build a live interactive dashboard with gradio.
The current time is refreshed every second and the plot every half second by using the 'every' keyword in the event handler.
Changing the value of the slider will control the period of the sine curve (the distance between peaks).
"""

# !pip install -q gradio numpy pandas plotly

"""# import em"""

import math

import pandas as pd

import gradio as gr
import datetime
import numpy as np
import cv2
import imageio
import tempfile
from PIL import Image
import torch
from google.colab import userdata
import os

token = userdata.get('HF_TOKEN')
os.environ['HF_TOKEN'] = token

print("HF_TOKEN environment variable has been set.")

from transformers import pipeline
from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL

controlnet = ControlNetModel.from_pretrained("diffusers/controlnet-depth-sdxl-1.0", variant="fp16", use_safetensors=True, torch_dtype=torch.float16)
vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16)

pipe = StableDiffusionXLControlNetPipeline.from_pretrained("stabilityai/stable-diffusion-xl-base-1.0", controlnet=controlnet, vae=vae, variant="fp16", use_safetensors=True, torch_dtype=torch.float16, token=token)
pipe.enable_model_cpu_offload()

from creative_image_generation.config import Config

depth_estimator = pipeline('depth-estimation', model="LiheYoung/depth-anything-small-hf")

# Global list to store captured frames
captured_frames = []
# Global number of images to capture
num_images_to_capture = 3
# Global duration in seconds over which to capture images
capture_duration = 2  # e.g., 6 seconds for 3 images, 2 seconds apart
# Global variables to track timing
capture_start_time = None
capture_interval = capture_duration / num_images_to_capture
last_capture_time = None
# Global variables to track default settings
default_prompt = "Multistory solid glass office building with white frames built by Norman Foster, with park and cherry blossoms in foreground"
default_seed = 7797676568
default_steps = 10

# Define a function to process video frames for timed capture

def timed_capture(frame):
    global captured_frames, num_images_to_capture, capture_start_time, capture_interval, last_capture_time
    import time
    now = time.time()
    # Crop to center square
    h, w = frame.shape[:2]
    min_dim = min(h, w)
    top = (h - min_dim) // 2
    left = (w - min_dim) // 2
    square_frame = frame[top:top+min_dim, left:left+min_dim]
    # Resize to 128x128
    square_frame = cv2.resize(square_frame, (128, 128), interpolation=cv2.INTER_AREA)
    if capture_start_time is None:
        capture_start_time = now
        last_capture_time = now - capture_interval  # So first frame is captured immediately
    # Only capture if we haven't reached the limit
    if len(captured_frames) < num_images_to_capture:
        # Capture at intervals
        if now - last_capture_time >= capture_interval:
            captured_frames.append(square_frame.copy())
            last_capture_time = now
            print(f"Captured image {len(captured_frames)} at {now - capture_start_time:.2f}s")
    # If we've captured enough images, return None to stop streaming
    if len(captured_frames) >= num_images_to_capture:
        return None
    # Otherwise, return the current frame
    return square_frame

# Custom function to display captured faces after streaming stops
with gr.Blocks(title="Live Webcam Feed with Timed Capture") as demo:
    captured_frames = []  # Reset for each new session
    capture_start_time = None
    last_capture_time = None

    gr.Markdown("# Timed Image Capture using OpenCV.\n This app captures a set number of images from your webcam, evenly spaced over a set duration, and displays them in a gallery.\n\n **Note:** Ensure your webcam is enabled and accessible by the browser.")
    # Settings row at the top
    with gr.Row():
        num_images_slider = gr.Slider(minimum=1, maximum=10, value=num_images_to_capture, step=1, label="Number of Images to Capture")
        duration_slider = gr.Slider(minimum=2, maximum=30, value=capture_duration, step=1, label="Capture Duration (seconds)")
        interval_display = gr.Markdown(f"**Interval:** {capture_interval:.2f} seconds", elem_id="interval-display")
        invert_depth_checkbox = gr.Checkbox(label="Invert Depth", value=False)
        depth_contrast_slider = gr.Slider(minimum=0.5, maximum=2.0, value=1.0, step=0.05, label="Depth Contrast")

    # Reset button row below settings
    with gr.Row():
        reset_button = gr.Button("Reset App (Images will be reset)", variant="stop")
    # User prompt text input under the reset button
    with gr.Row():
        user_prompt = gr.Textbox(label="User Prompt", value=default_prompt, placeholder="Enter a prompt for image processing", lines=2)
    with gr.Row():
        user_seed = gr.Textbox(label="Seed", value=str(default_seed), placeholder="Enter a seed for image processing", lines=1)
        user_steps = gr.Textbox(label="Steps", value=str(default_steps), placeholder="Enter number of steps for image processing", lines=1)

    with gr.Row() as webcam_row:
        webcam = gr.Image(
            sources=["webcam"],
            streaming=True,
            label="Webcam Input",
            width=300,
            height=300,
        )
        output = gr.Image(label="Current Output")

    row_height = 300
    gif_width = 200
    with gr.Row():
        images_gallery = gr.Gallery(label="Captured Images", visible=False, columns=num_images_to_capture, scale=1, height=row_height)
        images_gif = gr.Image(label="Images GIF", visible=False, scale=0, width=gif_width, height=row_height)
        images_count = gr.Markdown(f"# **Images captured:** 0/{num_images_to_capture}", visible=True)

    with gr.Row():
        processed_images_gallery = gr.Gallery(label="Processed Images", visible=False, columns=num_images_to_capture, scale=1, height=row_height)
        processed_gif = gr.Image(label="Processed GIF", visible=False, scale=0, width=gif_width, height=row_height)

    def images_to_gif(images, size=(128, 128)):
        if not images:
            return None
        # Convert PIL images to numpy arrays if needed
        resized_images = [cv2.resize(np.array(f), size) for f in images]
        with tempfile.NamedTemporaryFile(suffix=".gif", delete=False) as tmpfile:
            imageio.mimsave(tmpfile.name, resized_images, format='GIF', duration=0.6, loop=0)
            return tmpfile.name

    def process_images(images, invert_depth=False, depth_contrast=1.0, user_prompt=None, user_seed=None, user_steps=None):
        processed_images = []
        # Use user_prompt, user_seed, user_steps as strings
        Config.PROMPT = user_prompt if user_prompt else default_prompt
        Config.SEED = int(user_seed) if user_seed and str(user_seed).isdigit() else default_seed
        Config.STEPS = int(user_steps) if user_steps and str(user_steps).isdigit() else default_steps
        for img in images:
            # Convert OpenCV image (BGR) to PIL image (RGB)
            # if isinstance(img, np.ndarray):
            #     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            if not isinstance(img, Image.Image):
                pil_image = Image.fromarray(img)
                
            depth_image = depth_estimator(pil_image)['depth']
            depth_image_np = np.array(depth_image)
            depth_image_np = depth_image_np[:, :, None]
            depth_image_np = np.concatenate([depth_image_np, depth_image_np, depth_image_np], axis=2)
            depth_pil = Image.fromarray(depth_image_np)

            # depth_image_np = np.array(depth_image).astype(np.float32)
            # # Normalize to 0-1
            # ptp_val = np.ptp(depth_image_np)
            # depth_image_np = (depth_image_np - depth_image_np.min()) / (ptp_val + 1e-8)
            # # Apply contrast/stretch
            # mean = 0.5
            # depth_image_np = mean + (depth_image_np - mean) * depth_contrast
            # depth_image_np = np.clip(depth_image_np, 0, 1)
            # # Invert if needed
            # if invert_depth:
            #     depth_image_np = 1.0 - depth_image_np
            # depth_image_np = (depth_image_np * 255).astype(np.uint8)
            # depth_image_np = depth_image_np[:, :, None]
            # depth_image_np = np.concatenate([depth_image_np, depth_image_np, depth_image_np], axis=2)
            # depth_image_np = cv2.cvtColor(depth_image_np, cv2.COLOR_RGB2BGR)
            # Convert to PIL for pipeline
            # depth_pil = Image.fromarray(depth_image_np)

            scale = 0.5
            generator = torch.Generator(Config.TORCH_DEVICE).manual_seed(Config.SEED)
            image = pipe(Config.PROMPT, image=depth_pil, control_image=depth_pil, num_inference_steps=Config.STEPS, generator=generator, strength=0.99,controlnet_conditioning_scale=scale).images[0]

            processed_images.append(image)
        return processed_images

    def stream_callback(frame, invert_depth, depth_contrast, user_prompt, user_seed, user_steps):
        global captured_frames, capture_start_time, last_capture_time
        # Prevent repeated processing if already processed
        if getattr(stream_callback, "_already_processed", False):
            # Use last processed results if available
            if hasattr(stream_callback, "_last_results"):
                return stream_callback._last_results
            return [
                gr.update(visible=False, streaming=False),
                gr.update(visible=False),
                gr.update(visible=True, value=[]),
                gr.update(visible=True, value=None),
                gr.update(visible=False, value=f"# **Images captured:** 0/{num_images_to_capture}"),
                gr.update(visible=True, value=[]),
                gr.update(visible=True, value=None)
            ]
        result = timed_capture(frame)
        images_count_value = f"# **Images captured:** {len(captured_frames)}/{num_images_to_capture}"
        if result is None:
            print("No more images to capture.")
            # Store a copy before clearing
            captured_frames_copy = [img.copy() if isinstance(img, np.ndarray) else img for img in captured_frames]
            processed = process_images(captured_frames_copy, invert_depth, depth_contrast, user_prompt, user_seed, user_steps)
            images_gif_path = images_to_gif(captured_frames_copy)
            processed_gif_path = images_to_gif(processed)
            # Prepare results before clearing
            results = [
                gr.update(visible=False, streaming=False),  # Hide webcam
                gr.update(visible=False),  # Hide output
                gr.update(visible=True, value=captured_frames_copy),  # Show gallery with captured images
                gr.update(visible=True, value=images_gif_path),  # Show images GIF
                gr.update(visible=False, value=images_count_value),  # Show count
                gr.update(visible=True, value=processed),  # Show processed images gallery
                gr.update(visible=True, value=processed_gif_path)  # Show processed images GIF
            ]
            # Reset state after processing to avoid repeated calls
            captured_frames.clear()
            capture_start_time = None
            last_capture_time = None
            stream_callback._already_processed = True
            stream_callback._last_results = results
            return results
        # Reset the flag if capturing again
        stream_callback._already_processed = False
        stream_callback._last_results = None
        return [
            frame,  # Return the original frame
            result,  # Return the current frame
            gr.update(visible=False),  # Hide gallery if not capturing
            gr.update(visible=False),  # Hide GIF
            gr.update(visible=True, value=images_count_value),  # Show count
            gr.update(visible=False),  # Hide processed gallery
            gr.update(visible=False)   # Hide processed GIF
        ]

    def update_interval(num_images, duration):
        interval = duration / num_images if num_images > 0 else 0
        return gr.update(value=f"**Interval:** {interval:.2f} seconds")

    num_images_slider.change(update_interval, [num_images_slider, duration_slider], interval_display)
    duration_slider.change(update_interval, [num_images_slider, duration_slider], interval_display)

    def update_globals(num_images, duration):
        global num_images_to_capture, capture_duration, capture_interval
        num_images_to_capture = num_images
        capture_duration = duration
        capture_interval = duration / num_images if num_images > 0 else 0
        return None

    num_images_slider.change(update_globals, [num_images_slider, duration_slider], None)
    duration_slider.change(update_globals, [num_images_slider, duration_slider], None)

    def reset_all():
        global captured_frames, capture_start_time, last_capture_time
        captured_frames = []
        capture_start_time = None
        last_capture_time = None
        images_count_value = f"# **Images captured:** 0/{num_images_to_capture}"
        return [
            gr.update(visible=True, streaming=True),  # Show webcam
            gr.update(visible=False),  # Hide output
            gr.update(visible=False, value=[]),  # Hide gallery
            gr.update(visible=False, value=None),  # Hide GIF
            gr.update(visible=True, value=images_count_value),  # Reset count
            gr.update(visible=False, value=[]),  # Hide processed gallery
            gr.update(visible=False, value=None)  # Hide processed GIF
        ]

    reset_button.click(
        reset_all,
        inputs=None,
        outputs=[webcam, output, images_gallery, images_gif, images_count, processed_images_gallery, processed_gif]
    )

    webcam.stream(
        fn=stream_callback,
        inputs=[webcam, invert_depth_checkbox, depth_contrast_slider, user_prompt, user_seed, user_steps],
        outputs=[webcam, output, images_gallery, images_gif, images_count, processed_images_gallery, processed_gif]
    )

def run_demo():
    demo.launch(share=True, debug=True)
